{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_dataset = load_dataset(\"yitingxie/rlhf-reward-datasets\", split=[\"test\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset = list(raw_dataset[0])\n",
    "raw_dataset[:3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import Mapping\n",
    "from llama_likes import Completion, PreferenceInput\n",
    "\n",
    "CHOSEN = \"chosen\"\n",
    "REJECTED = \"rejected\"\n",
    "\n",
    "\n",
    "def to_preference_input(entry: Mapping[str, str]) -> PreferenceInput:\n",
    "\n",
    "    def build_completions(entry: Mapping[str, str]) -> tuple[Completion, Completion]:\n",
    "        items = [\n",
    "            Completion(player_id=CHOSEN, completion=entry[CHOSEN]),\n",
    "            Completion(player_id=REJECTED, completion=entry[REJECTED])\n",
    "        ]\n",
    "        random.shuffle(items)\n",
    "        return tuple(items)\n",
    "\n",
    "    completion_a, completion_b = build_completions(entry)\n",
    "    return PreferenceInput(\n",
    "        instruction=entry[\"prompt\"],\n",
    "        completion_a=completion_a,\n",
    "        completion_b=completion_b,\n",
    "    )\n",
    "\n",
    "\n",
    "data: list[PreferenceInput] = [to_preference_input(d) for d in raw_dataset]\n",
    "data[:5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Briefly transforming these into a format that is digestible to our comparison mechanism later..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_likes import Payoff, PreferenceResult\n",
    "\n",
    "def get_payoff(pref_input: PreferenceInput) -> Payoff:\n",
    "    if pref_input.completion_a.player_id == CHOSEN:\n",
    "        return Payoff.PLAYER_A_WINS\n",
    "    return Payoff.PLAYER_B_WINS\n",
    "\n",
    "human_labeled = [PreferenceResult(\n",
    "    preference_input=pref_input,\n",
    "    payoff=get_payoff(pref_input)\n",
    ") for pref_input in data]\n",
    "\n",
    "human_labeled[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labeling a few examples with GPT..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "from llama_likes import OpenaiModel, OpenaiRanker, PreferenceError\n",
    "\n",
    "TEST_SAMPLE_SIZE = 10\n",
    "\n",
    "openai_model = OpenaiModel.GPT_4\n",
    "openai_ranker = OpenaiRanker(openai_model)\n",
    "\n",
    "labels: list[Union[PreferenceResult, PreferenceError]] = []\n",
    "for input in data[:TEST_SAMPLE_SIZE]:\n",
    "    label = openai_ranker.rank(input)\n",
    "    labels.append(label)\n",
    "\n",
    "labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [l for l in labels if isinstance(l, PreferenceResult)]\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Mapping, Sequence\n",
    "from sklearn.isotonic import spearmanr\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "\n",
    "def calculate_cohens_kappa(data1: Sequence[int], data2: Sequence[int]) -> float:\n",
    "    \"\"\"\n",
    "    Calculate Cohen's Kappa for two sets of categorical data.\n",
    "    \n",
    "    Args:\n",
    "        data1 (Sequence[int]): Categorical data from method 1.\n",
    "        data2 (Sequence[int]): Categorical data from method 2.\n",
    "    \n",
    "    Returns:\n",
    "        float: Cohen's Kappa score.\n",
    "    \"\"\"\n",
    "    return cohen_kappa_score(data1, data2)\n",
    "\n",
    "\n",
    "def calculate_spearmans_correlation(data1: Sequence[int], data2: Sequence[int]) -> float:\n",
    "    \"\"\"\n",
    "    Calculate Spearman's rank correlation coefficient for two sets of ordinal data.\n",
    "    \n",
    "    Args:\n",
    "        data1 (Sequence[int]): Ratings from the first rater.\n",
    "        data2 (Sequence[int]): Ratings from the second rater.\n",
    "    \n",
    "    Returns:\n",
    "        float: Spearman's rank correlation coefficient.\n",
    "    \"\"\"\n",
    "    correlation, _p_value = spearmanr(data1, data2)\n",
    "    return correlation\n",
    "\n",
    "\n",
    "def transform_payoff(payoff: tuple[float, float]) -> int:\n",
    "    mapping = {(1, 0): 0, (0.5, 0.5): 1, (0, 1): 2}\n",
    "    return mapping[payoff]\n",
    "\n",
    "\n",
    "def compare_ai_and_human(labels1: Sequence[PreferenceResult], labels2: Sequence[PreferenceResult]) -> Mapping[str, float]:\n",
    "    data1 = [transform_payoff(l.payoff.value) for l in labels1]\n",
    "    print(data1)\n",
    "    data2 = [transform_payoff(l.payoff.value) for l in labels2]\n",
    "    print(data2)\n",
    "    \n",
    "    ck = cohen_kappa_score(data1, data2)\n",
    "    sr = calculate_spearmans_correlation(data1, data2)\n",
    "    return {\n",
    "        \"ck\": ck,\n",
    "        \"sr\": sr\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = compare_ai_and_human(human_labeled[:TEST_SAMPLE_SIZE], labels)\n",
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama-likes-0e_Q1Xay-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
